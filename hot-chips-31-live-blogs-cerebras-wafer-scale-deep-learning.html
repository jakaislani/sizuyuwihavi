<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Cerebras' 1.2 Trillion Transistor Deep Learning Processor - SyncVibe</title><script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta name=robots content="index,follow,noarchive"><meta property="og:title" content="Cerebras' 1.2 Trillion Transistor Deep Learning Processor"><meta property="og:description" content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta property="og:type" content="article"><meta property="og:url" content="/hot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-05-17T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-17T00:00:00+00:00"><meta itemprop=name content="Cerebras' 1.2 Trillion Transistor Deep Learning Processor"><meta itemprop=description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta itemprop=datePublished content="2024-05-17T00:00:00+00:00"><meta itemprop=dateModified content="2024-05-17T00:00:00+00:00"><meta itemprop=wordCount content="973"><meta itemprop=keywords content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/mainroad/css/style.css><link rel="shortcut icon" href=./favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=./index.html title=SyncVibe rel=home><div class="logo__item logo__text"><div class=logo__title>SyncVibe</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Cerebras' 1.2 Trillion Transistor Deep Learning Processor</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2024-05-17T00:00:00Z>May 17, 2024</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=./categories/blog/ rel=category>blog</a></span></div></div></header><div class="content post__content clearfix"><p><a href=# id=post0819204952><span class=lb_time>08:49PM EDT</span></a> - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology.</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175010_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205102><span class=lb_time>08:51PM EDT</span></a> - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175016_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205119><span class=lb_time>08:51PM EDT</span></a> - TSMC 16nm</p><p><a href=# id=post0819205137><span class=lb_time>08:51PM EDT</span></a> - 215mm x 215mm - 8.5 inches per side</p><p><a href=# id=post0819205152><span class=lb_time>08:51PM EDT</span></a> - 56 times larger than the largest GPU today</p><p><a href=# id=post0819205227><span class=lb_time>08:52PM EDT</span></a> - Built for Deep Learning</p><p><a href=# id=post0819205231><span class=lb_time>08:52PM EDT</span></a> - DL training is hard (ed: this is an understatement)</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175210_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205255><span class=lb_time>08:52PM EDT</span></a> - Peta-to-exa scale compute range</p><p><a href=# id=post0819205302><span class=lb_time>08:53PM EDT</span></a> - The shape of the problem is difficult to scale</p><p><a href=# id=post0819205311><span class=lb_time>08:53PM EDT</span></a> - Fine grain has a lot of parallelism</p><p><a href=# id=post0819205317><span class=lb_time>08:53PM EDT</span></a> - Coarse grain is inherently serial</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175236_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205327><span class=lb_time>08:53PM EDT</span></a> - Training is the process of applying small changes, serially</p><p><a href=# id=post0819205337><span class=lb_time>08:53PM EDT</span></a> - Size and shape of the problem makes training NN really hard</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175342_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205358><span class=lb_time>08:53PM EDT</span></a> - Today we have dense vector compute</p><p><a href=# id=post0819205416><span class=lb_time>08:54PM EDT</span></a> - For Coarse Grained, require high speed interconnect to run mutliple instances. Still limited</p><p><a href=# id=post0819205422><span class=lb_time>08:54PM EDT</span></a> - Scaling is limited and costly</p><p><a href=# id=post0819205455><span class=lb_time>08:54PM EDT</span></a> - Specialized accelerators are the answer</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175439_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205504><span class=lb_time>08:55PM EDT</span></a> - NN: what is the right architecture</p><p><a href=# id=post0819205528><span class=lb_time>08:55PM EDT</span></a> - Need a core to be optimized for NN primitives</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175510_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205532><span class=lb_time>08:55PM EDT</span></a> - Need a programmable NN core</p><p><a href=# id=post0819205538><span class=lb_time>08:55PM EDT</span></a> - Needs to do sparse compute fast</p><p><a href=# id=post0819205542><span class=lb_time>08:55PM EDT</span></a> - Needs fast local memory</p><p><a href=# id=post0819205552><span class=lb_time>08:55PM EDT</span></a> - All of the cores should be connected with a fast interconnect</p><p><a href=# id=post0819205620><span class=lb_time>08:56PM EDT</span></a> - Cerebras uses flexible cores. Flexible general ops for control processing</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175559_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205629><span class=lb_time>08:56PM EDT</span></a> - Core should handle tensor operations very efficiency</p><p><a href=# id=post0819205637><span class=lb_time>08:56PM EDT</span></a> - Forms the bulk fo the compute in any neural network</p><p><a href=# id=post0819205645><span class=lb_time>08:56PM EDT</span></a> - Tensors as first class operands</p><p><a href=# id=post0819205720><span class=lb_time>08:57PM EDT</span></a> - fmac native op</p><p><a href=# id=post0819205758><span class=lb_time>08:57PM EDT</span></a> - NN naturally creates sparse networks</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175738_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205810><span class=lb_time>08:58PM EDT</span></a> - The core has native sparse processing in the hardware with dataflow scheduling</p><p><a href=# id=post0819205815><span class=lb_time>08:58PM EDT</span></a> - All the compute is triggered by the data</p><p><a href=# id=post0819205823><span class=lb_time>08:58PM EDT</span></a> - Filters all the sparse zeros, and filters the work</p><p><a href=# id=post0819205838><span class=lb_time>08:58PM EDT</span></a> - saves the power and energy, and get performance and acceleration by moving onto the next useful work</p><p><a href=# id=post0819205848><span class=lb_time>08:58PM EDT</span></a> - Enabled because arch has fine-grained execution datapaths</p><p><a href=# id=post0819205855><span class=lb_time>08:58PM EDT</span></a> - Many small cores with independent instructions</p><p><a href=# id=post0819205901><span class=lb_time>08:59PM EDT</span></a> - Allows for very non-uniform work</p><p><a href=# id=post0819205905><span class=lb_time>08:59PM EDT</span></a> - Next is memory</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175910_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205934><span class=lb_time>08:59PM EDT</span></a> - Traditional memory architectures are not optimized for DL</p><p><a href=# id=post0819205946><span class=lb_time>08:59PM EDT</span></a> - Traditional memory requires high data reuse for performane</p><p><a href=# id=post0819210004><span class=lb_time>09:00PM EDT</span></a> - Normal matrix multiply has low end data reuse</p><p><a href=# id=post0819210028><span class=lb_time>09:00PM EDT</span></a> - Translating Mat*Vec into Mat*Mat, but changes the training dynamics</p><p><a href=# id=post0819210044><span class=lb_time>09:00PM EDT</span></a> - Cerebras has high-perf, fully distributed on-chip SRAM next to the cores</p><p><a href=# id=post0819210106><span class=lb_time>09:01PM EDT</span></a> - Getting orders of magnitude higher bandwidth</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180051_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210120><span class=lb_time>09:01PM EDT</span></a> - ML can be done the way it wants to be done</p><p><a href=# id=post0819210141><span class=lb_time>09:01PM EDT</span></a> - High bandwidth, low latency interconnect</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180126_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210147><span class=lb_time>09:01PM EDT</span></a> - fast and fully configurable fabric</p><p><a href=# id=post0819210158><span class=lb_time>09:01PM EDT</span></a> - all hw based communication avoicd sw overhead</p><p><a href=# id=post0819210205><span class=lb_time>09:02PM EDT</span></a> - 2D mesh topology</p><p><a href=# id=post0819210218><span class=lb_time>09:02PM EDT</span></a> - higher utlization and efficiency than global topologies</p><p><a href=# id=post0819210248><span class=lb_time>09:02PM EDT</span></a> - Need more than a single die</p><p><a href=# id=post0819210254><span class=lb_time>09:02PM EDT</span></a> - Solition is a wafer scale</p><p><a href=# id=post0819210315><span class=lb_time>09:03PM EDT</span></a> - Build Big chips</p><p><a href=# id=post0819210321><span class=lb_time>09:03PM EDT</span></a> - Cluster scale perf on a single chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180258_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210330><span class=lb_time>09:03PM EDT</span></a> - GB of fast memory (SRAM) 1 clock cycle from the core</p><p><a href=# id=post0819210336><span class=lb_time>09:03PM EDT</span></a> - That's impossible with off-chip memory</p><p><a href=# id=post0819210343><span class=lb_time>09:03PM EDT</span></a> - Full on-chip interconnect fabric</p><p><a href=# id=post0819210356><span class=lb_time>09:03PM EDT</span></a> - Model parallel, linear performance scaling</p><p><a href=# id=post0819210406><span class=lb_time>09:04PM EDT</span></a> - Map the entire neural network onto the chip at once</p><p><a href=# id=post0819210418><span class=lb_time>09:04PM EDT</span></a> - One instance of NN, don't have to increase batch size to get cluster scale perf</p><p><a href=# id=post0819210425><span class=lb_time>09:04PM EDT</span></a> - Vastly lower power and less space</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180435_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210457><span class=lb_time>09:04PM EDT</span></a> - Can use TensorFlow and PyTorch</p><p><a href=# id=post0819210512><span class=lb_time>09:05PM EDT</span></a> - Performs placing and routing to map neural network layers to fabric</p><p><a href=# id=post0819210522><span class=lb_time>09:05PM EDT</span></a> - Entire wafer operates on the single neural network</p><p><a href=# id=post0819210529><span class=lb_time>09:05PM EDT</span></a> - Challenges of wafer scale</p><p><a href=# id=post0819210557><span class=lb_time>09:05PM EDT</span></a> - Need cross-die connectivity, yield, thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180534_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180630_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210655><span class=lb_time>09:06PM EDT</span></a> - Scribe line separates the die. On top of the scribe line, create wires</p><p><a href=# id=post0819210702><span class=lb_time>09:07PM EDT</span></a> - Extends 2D mesh fabric across all die</p><p><a href=# id=post0819210711><span class=lb_time>09:07PM EDT</span></a> - Same connectivity between cores and between die</p><p><a href=# id=post0819210732><span class=lb_time>09:07PM EDT</span></a> - More efficient than off-chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180719_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210747><span class=lb_time>09:07PM EDT</span></a> - Full BW at the die level</p><p><a href=# id=post0819210810><span class=lb_time>09:08PM EDT</span></a> - Redundancy helps yield</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180753_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210831><span class=lb_time>09:08PM EDT</span></a> - Redundant cores and redundant fabric links</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180816_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210848><span class=lb_time>09:08PM EDT</span></a> - Reconnect the fabric with links</p><p><a href=# id=post0819210855><span class=lb_time>09:08PM EDT</span></a> - Drive yields high</p><p><a href=# id=post0819210902><span class=lb_time>09:09PM EDT</span></a> - Transparent to software</p><p><a href=# id=post0819210924><span class=lb_time>09:09PM EDT</span></a> - Thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180909_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210935><span class=lb_time>09:09PM EDT</span></a> - Normal tech, too much mechanical stress via thermal expansion</p><p><a href=# id=post0819210939><span class=lb_time>09:09PM EDT</span></a> - Custom connector developed</p><p><a href=# id=post0819210948><span class=lb_time>09:09PM EDT</span></a> - Connector absorbs the variation in thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181005_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211026><span class=lb_time>09:10PM EDT</span></a> - All components need to be held with precise alignment - custom packaging tools</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181031_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211049><span class=lb_time>09:10PM EDT</span></a> - Power and Cooling</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181056_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211117><span class=lb_time>09:11PM EDT</span></a> - Power planes don't work - isn't enough copper in the PCB to do it that way</p><p><a href=# id=post0819211128><span class=lb_time>09:11PM EDT</span></a> - Heat density too high for direct air cooling</p><p><a href=# id=post0819211206><span class=lb_time>09:12PM EDT</span></a> - Bring current perpendicular to the wafer. Water cooled perpendicular too</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181142_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181224_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211406><span class=lb_time>09:14PM EDT</span></a> - Q&A Time</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181341_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211445><span class=lb_time>09:14PM EDT</span></a> - Q and A</p><p><a href=# id=post0819211458><span class=lb_time>09:14PM EDT</span></a> - Already in use? Yes</p><p><a href=# id=post0819211515><span class=lb_time>09:15PM EDT</span></a> - Can you make a round chip? Square is more convenient</p><p><a href=# id=post0819211558><span class=lb_time>09:15PM EDT</span></a> - Yield? Mature processes are quite good and uniform</p><p><a href=# id=post0819211658><span class=lb_time>09:16PM EDT</span></a> - Does it cost less than a house? Everything is amortised across the wafer</p><p><a href=# id=post0819211722><span class=lb_time>09:17PM EDT</span></a> - Regular processor for housekeeping? They can all do it</p><p><a href=# id=post0819211741><span class=lb_time>09:17PM EDT</span></a> - Is it fully synchronous? No</p><p><a href=# id=post0819212001><span class=lb_time>09:20PM EDT</span></a> - Clock rate? Not disclosed</p><p><a href=# id=post0819212042><span class=lb_time>09:20PM EDT</span></a> - That's a wrap. Next is Habana</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH51g5RxZqGnpGKwqbXPrGRsaV2htrexjJujqJ%2BjYrCmvsSbqZqrXayup7HRZqqcmZyaeqWxxKlkpZ2Rp7uqusY%3D</p></div></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=./how-long-do-automatic-watches-last.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>How long do automatic watches last?</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=./samsung-starts-production-of-lpddr55500-devices-12-gb-of-dram-in-a-smartphone.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>12 GB of DRAM in a Smartphone</p></a></div></nav></div><aside class=sidebar><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=./cast-and-crew.html>Amalia Yoo movie reviews &amp;amp; film summaries</a></li><li class=widget__item><a class=widget__link href=./how-do-dogs-get-parvo-from-the-ground.html>How do dogs get parvo from the ground?</a></li><li class=widget__item><a class=widget__link href=./how-many-licks-get-center-tootsie-pop.html>How Many Licks to Get to the Center of a Tootsie Pop: The Real Answer!</a></li><li class=widget__item><a class=widget__link href=./the-grab-documentary-film-review-2024.html>The Grab movie review &amp;amp; film summary (2024)</a></li><li class=widget__item><a class=widget__link href=./jonathan-daviss-outer-banks-movies-dad.html>Jonathan Daviss; Outer Banks, Movies, Dad, Girlfriend, Height, Facts</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=./categories/blog/>blog</a></li></ul></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2024 SyncVibe.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=https://assets.cdnweb.info/hugo/mainroad/js/menu.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>